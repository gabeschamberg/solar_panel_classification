{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Solar Image Classification - Basic CNN\n",
    "\n",
    "The goal of this notebook is to classify from satellite images whether or not a home has solar panels installed. The creation of this notebook relied heavily on a [Keras blog post](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) about building neural networks with a small amount of data. To start, let's import packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os # for navigating directories\n",
    "import numpy as np\n",
    "from skimage import io # used to load images as numpy arrays\n",
    "from sklearn.model_selection import train_test_split # split a data set into training and testing\n",
    "from scipy.misc import imresize, imsave # resize images and save as pngs\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from keras.models import Model,load_model # basic class for specifying and training a neural network\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Flatten # all the various neural network layers\n",
    "from keras.utils import np_utils # utilities for one-hot encoding of ground truth value\n",
    "from keras.optimizers import RMSprop # optimization algorithm to use to search for optimal weights\n",
    "from keras import regularizers # used to regularize the weights\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img # for data augmentation\n",
    "\n",
    "import keras.backend as K # used for modifying learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next load the data as numpy arrays of the desired size. Of the image sizes I tried, I found 30 x 30 pixels to work best. Given the small amount of data, we want to minimize the number of parameters we need to estimate. Therefore, the smaller we can have the photo (and still be able to distinguish the pool), the better. In order to be sure we aren't setting the resolution too low, the first few images in each class are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "force_same = False\n",
    "\n",
    "# Start by counting the number of photos in each class\n",
    "panel_count = 0\n",
    "for (dirpath, dirnames, filenames) in os.walk('../data/original/with panels'):\n",
    "    for f in filenames:\n",
    "        panel_count += 1\n",
    "\n",
    "no_panel_count = 0\n",
    "for (dirpath, dirnames, filenames) in os.walk('../data/original/without panels'):\n",
    "    for f in filenames:\n",
    "        no_panel_count += 1\n",
    "        # for now we'll ensure the same number of pool/no pool examples to avoid biasing the model\n",
    "        if (no_panel_count == panel_count) and force_same:\n",
    "            break\n",
    "\n",
    "total_count = panel_count + no_panel_count\n",
    "\n",
    "# desired shape of the input images\n",
    "shape = (64,64,3)\n",
    "\n",
    "# numpy arrays for storing data and labels (first dimension indexes over samples)\n",
    "X = np.zeros((total_count,)+shape)\n",
    "y = np.zeros((total_count,))\n",
    "# because of the order we are loading them, the first ones are all pool examples\n",
    "y[:panel_count] = 1\n",
    "\n",
    "index = 0\n",
    "for (dirpath, dirnames, filenames) in os.walk('../data/original/with panels'):\n",
    "    for f in filenames:\n",
    "        # load the image as numpy array\n",
    "        im = io.imread('../data/original/with panels/' + f)\n",
    "        # resize image to desired shape\n",
    "        im_resize = imresize(im,shape)\n",
    "        # store in data matrix\n",
    "        X[index,:,:,:] = im_resize\n",
    "        index += 1\n",
    "        if index < 4:\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.imshow(im_resize)\n",
    "            plt.title(f)\n",
    "            plt.show()\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os.walk('../data/original/without panels'):\n",
    "    for f in filenames:\n",
    "        im = io.imread('../data/original/without panels/' + f)\n",
    "        im_resize = imresize(im,shape)\n",
    "        X[index,:,:,:] = im_resize\n",
    "        index += 1\n",
    "        if index - panel_count < 4:\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.imshow(im_resize)\n",
    "            plt.title(f)\n",
    "            plt.show()\n",
    "        if index == total_count:\n",
    "            break\n",
    "        \n",
    "print(\"%i Panel Examples\"%panel_count)\n",
    "print(\"%i No Panel Examples\"%no_panel_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that the complete data and labels are loaded we can split them into testing and training sets, store important values, and prepare the data for use by Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train on 2/3 and test on 1/3 of data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,random_state=4)\n",
    "\n",
    "# store the details\n",
    "num_train, height, width, depth = X_train.shape\n",
    "num_test = X_test.shape[0]\n",
    "\n",
    "# ensure input data is correct type\n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "#normalize input data\n",
    "X_train /= np.max(X_train)\n",
    "X_test /= np.max(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we specify all the parameters used to create the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_num = 1 # can set this for saving models\n",
    "batch_size = 5 # in each iteration, we consider 10 training examples at once\n",
    "kernel_size = 12 # we will use 3x3 kernels throughout\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth_1 = 128 # we will initially have 8 kernels per conv. layer...\n",
    "conv_depth_2 = 256 # ...switching to 16 after the first pooling layer\n",
    "drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
    "drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
    "hidden_size = 512 # the FC layer will have 32 neurons\n",
    "conv_reg = 0.00001 # amount of regularization to perform in convolutional layers (l2)\n",
    "hidden_reg = 0.00001 # amount of regularization to perform in the hidden layer (l2)\n",
    "learning_rate = 0.00001 # learning rate for optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "conv_1 = Convolution2D(conv_depth_1, (kernel_size, kernel_size),\n",
    "                       kernel_regularizer=regularizers.l2(conv_reg),\n",
    "                       padding='same', activation='relu')(inp)\n",
    "conv_2 = Convolution2D(conv_depth_1, (kernel_size, kernel_size),\n",
    "                       kernel_regularizer=regularizers.l2(conv_reg),\n",
    "                       padding='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "conv_3 = Convolution2D(conv_depth_2, (kernel_size, kernel_size),\n",
    "                       kernel_regularizer=regularizers.l2(conv_reg),\n",
    "                       padding='same', activation='relu')(drop_1)\n",
    "conv_4 = Convolution2D(conv_depth_2, (kernel_size, kernel_size),\n",
    "                       kernel_regularizer=regularizers.l2(conv_reg),\n",
    "                       padding='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_4)\n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "# Now flatten to 1D, apply FC -> ReLU (with dropout) -> softmax\n",
    "flat = Flatten()(drop_2)\n",
    "hidden1 = Dense(hidden_size, activation='relu',kernel_regularizer=regularizers.l2(hidden_reg))(flat)\n",
    "drop_3 = Dropout(drop_prob_2)(hidden1)\n",
    "out = Dense(1, activation='sigmoid')(drop_3)\n",
    "\n",
    "# To define a model, just specify its input and output layers\n",
    "model = Model(inputs=inp, outputs=out) \n",
    "\n",
    "# In order to specify learning rate, need to create an optimizer\n",
    "rms_prop = RMSprop(lr=learning_rate)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=rms_prop,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, increase the number of samples by augmenting our training set. First, create the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    rotation_range=15.,\n",
    "    width_shift_range=0.,\n",
    "    height_shift_range=0.,\n",
    "    shear_range=0.,\n",
    "    zoom_range=0.1,\n",
    "    channel_shift_range=0.,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rescale=None,\n",
    "    preprocessing_function=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate the augmented data and combine with the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "augmented_data = np.zeros((0,) + shape)\n",
    "augmented_labels = np.asarray([])\n",
    "\n",
    "# set the number of times to augment each image\n",
    "factor_increase = 5\n",
    "i = 0\n",
    "for batch_data,batch_labels in datagen.flow(X_train, y_train, batch_size=X_train.shape[0]):\n",
    "    augmented_data = np.vstack((augmented_data,batch_data))\n",
    "    augmented_labels = np.append(augmented_labels,batch_labels)\n",
    "    if i == factor_increase:\n",
    "        break\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "X_aug_train = np.vstack((X_train,augmented_data))\n",
    "y_aug_train = np.append(y_train,augmented_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model. I've found it easiest to just run one epoch at a time because running on verbose can sometimes freeze the notebook but running not verbose makes it hard to know the progress. This way you can print results after some epochs, save the model after some epochs, and update the learning rate after some epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(400):\n",
    "    fit_aug_results = model.fit(X_aug_train, y_aug_train, # Train the model using the training set...\n",
    "              batch_size=3, epochs=1,\n",
    "              verbose=0, validation_split=0.1,class_weight={0:1,1:2}) # ...holding out 10% of the data for validation\n",
    "    if(i%10==0):\n",
    "        print('*****Epoch %i*****'%i)\n",
    "        print('Training Loss: %f'%fit_aug_results.history['loss'][0])\n",
    "        print('Validation Loss: %f'%fit_aug_results.history['val_loss'][0])\n",
    "        print('Training Accuracy: %f'%fit_aug_results.history['acc'][0])\n",
    "        print('Validation Accuracy: %f'%fit_aug_results.history['val_acc'][0])\n",
    "    if(i%20==0):\n",
    "        print('------Saving Epoch %i------'%i)\n",
    "        model.save('../models/solar_model_%i_epoch%i.h5'%(model_num,i))\n",
    "    if(i%200==0):\n",
    "        K.set_value(model.optimizer.lr,0.000001)\n",
    "    if(i%300==0):\n",
    "        K.set_value(model.optimizer.lr,0.0000005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the model you want to analyze and check out the performance. Note that the portion below here can be run in a different notebook if the above cell is still running but has saved some models already. First let's look at the summary of results from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model('../models/solar_model_1_epoch200.h5')\n",
    "\n",
    "# predict based on test set\n",
    "y_hat = model.predict(X_test,verbose=1)\n",
    "# convert probabilities to True/False estimates\n",
    "labels = (y_hat > 0.5)\n",
    "# convert True/False to 1/0\n",
    "y_hat = [label[0] for label in labels.astype(int)]\n",
    "\n",
    "print(\"Test Accuracy: %f\"%(float(sum(y_hat == y_test))/len(y_hat)))\n",
    "print(\"Chance Accuracy: %f\"%(1-float(sum(y_test))/len(y_hat)))\n",
    "print(\"Solar Examples:%i\"%sum(y_test == 1))\n",
    "print(\"Misses:%i\"%sum(y_hat - y_test == -1))\n",
    "print(\"No Solar Examples:%i\"%sum(y_test == 0))\n",
    "print(\"False Alarms:%i\"%sum(y_hat - y_test == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's loop through each test sample and look at the probability it assigned. If the classification was incorrect, display the image so we can get an idea of what sort of images cause errors. If the classification is correct with above 95% certainty, then save that sample so we can look at how it passes through the network to see what kind of features are being generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep track of samples to look more closely at\n",
    "X_look = np.zeros_like(X_test)\n",
    "y_look = np.zeros_like(y_test)\n",
    "j = 0\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    x_i = X_test[i,:,:,:]\n",
    "    y_i = model.predict(np.expand_dims(x_i,axis=0))[0]\n",
    "    label_i = int(y_i>0.5)\n",
    "    if np.abs(y_i - y_test[i]) < 0.05:\n",
    "        X_look[j,:,:,:] = X_test[i,:,:,:]\n",
    "        y_look[j] = y_test[i]\n",
    "        j += 1\n",
    "    if label_i == y_test[i]:\n",
    "        outcome = 'Success'\n",
    "    else:\n",
    "        outcome = 'Failure'\n",
    "    print('%f - %s'%(y_i,outcome))\n",
    "    if y_test[i] != label_i:\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(x_i)\n",
    "        plt.title('P(panel)=%f - %s'%(y_i,outcome))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we will visualize the network by creating intermediate layer models whose outputs are the outputs of the convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create two intermediate layer models models\n",
    "intermediate_layer_model1 = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[1].output)\n",
    "\n",
    "intermediate_layer_model2 = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[5].output)\n",
    "\n",
    "# for the first ten samples in our 95% confidence group\n",
    "for index in range(10):\n",
    "    # display the image in question\n",
    "    im_resize = X_look[index,:,:,:]\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(im_resize)\n",
    "    plt.title(f)\n",
    "\n",
    "    # calling predict on the intermediate model gives the filter outputs\n",
    "    out1 = intermediate_layer_model1.predict(np.expand_dims(im_resize,axis=0))   \n",
    "    fig,axarr = plt.subplots(6,6,figsize=(10,10))\n",
    "    plt.title('layer 2 filter outputs')\n",
    "    axarr = axarr.reshape(-1)\n",
    "    for i in range(36):\n",
    "        filt = axarr[i].imshow(np.squeeze(out1,axis=0)[:,:,i])\n",
    "        axarr[i].set_title('max value: %5.2f'%(filt.get_clim()[1]))                                   \n",
    "        axarr[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    out2 = intermediate_layer_model2.predict(np.expand_dims(im_resize,axis=0))\n",
    "    fig,axarr = plt.subplots(6,6,figsize=(10,10))\n",
    "    plt.title('layer 6 filter outputs')\n",
    "    axarr = axarr.reshape(-1)\n",
    "    for i in range(36):\n",
    "        filt = axarr[i].imshow(np.squeeze(out2,axis=0)[:,:,i])\n",
    "        axarr[i].set_title('max value: %5.2f'%(filt.get_clim()[1]))                                   \n",
    "        axarr[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
